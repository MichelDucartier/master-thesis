@article{heitmann_deepbreathautomated_2023,
    title = {{DeepBreath}—automated detection of respiratory pathology from lung
             auscultation in 572 pediatric outpatients across 5 countries},
    volume = {6},
    copyright = {2023 The Author(s)},
    issn = {2398-6352},
    url = {https://www.nature.com/articles/s41746-023-00838-3},
    doi = {10.1038/s41746-023-00838-3},
    number = {1},
    journal = {npj Digital Medicine},
    author = {Heitmann, Julien and Glangetas, Alban and Doenz, Jonathan and
              Dervaux, Juliane and Shama, Deeksha M. and Garcia, Daniel Hinjos
              and Benissa, Mohamed Rida and Cantais, Aymeric and Perez, Alexandre
              and Müller, Daniel and Chavdarova, Tatjana and Ruchonnet-Metrailler
              , Isabelle and Siebert, Johan N. and Lacroix, Laurence and Jaggi,
              Martin and Gervaix, Alain and Hartley, Mary-Anne},
    month = jun,
    year = {2023},
    keywords = {Paediatrics, Respiratory signs and symptoms, Respiratory tract
                diseases},
    pages = {1--12},
}


@article{naumova_mythisyourthat_2024,
    title = {{MyThisYourThat} for interpretable identification of systematic
             bias in federated learning for biomedical images},
    volume = {7},
    copyright = {2024 The Author(s)},
    issn = {2398-6352},
    url = {https://www.nature.com/articles/s41746-024-01226-1},
    doi = {10.1038/s41746-024-01226-1},
    number = {1},
    journal = {npj Digital Medicine},
    author = {Naumova, Klavdiia and Devos, Arnout and Karimireddy, Sai Praneeth
              and Jaggi, Martin and Hartley, Mary-Anne},
    month = sep,
    year = {2024},
    keywords = {Medical imaging, Health care, Computer science, Biomedical
                engineering, Computational science},
    pages = {1--10},
}


@article{mannarini_what_2022,
    title = {What {If}…? {Pandemic} policy-decision-support to guide a
             cost-benefit-optimised, country-specific response},
    volume = {2},
    issn = {2767-3375},
    shorttitle = {What {If}…?},
    url = {
           https://journals.plos.org/globalpublichealth/article?id=10.1371/journal.pgph.0000721
           },
    doi = {10.1371/journal.pgph.0000721},
    number = {8},
    journal = {PLOS Global Public Health},
    author = {Mannarini, Giorgio and Posa, Francesco and Bossy, Thierry and
              Massemin, Lucas and Fernandez-Castanon, Javier and Chavdarova,
              Tatjana and Cañas, Pablo and Gupta, Prakhar and Jaggi, Martin and
              Hartley, Mary-Anne},
    month = aug,
    year = {2022},
    keywords = {COVID 19, Deep learning, Geography, Pandemics, Public and
                occupational health, Public policy, Switzerland, Unemployment
                rates},
    pages = {e0000721},
}

@misc{pineau2020improvingreproducibilitymachinelearning,
    title = {Improving Reproducibility in Machine Learning Research (A Report
             from the NeurIPS 2019 Reproducibility Program)},
    author = {Joelle Pineau and Philippe Vincent-Lamarre and Koustuv Sinha and
              Vincent Larivière and Alina Beygelzimer and Florence d'Alché-Buc
              and Emily Fox and Hugo Larochelle},
    year = {2020},
    eprint = {2003.12206},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2003.12206},
}

@misc{magnusson2023reproducibilitynlplearnedchecklist,
    title = {Reproducibility in NLP: What Have We Learned from the Checklist?},
    author = {Ian Magnusson and Noah A. Smith and Jesse Dodge},
    year = {2023},
    eprint = {2306.09562},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/2306.09562},
}

@article{naturereproducibility2016,
    title = {1,500 scientists lift the lid on reproducibility},
    author = {Monya Baker },
    year = {2016},
    doi = {https://doi.org/10.1038/533452a},
}

@article{li2024llava,
    title = {LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large
             Multimodal Models},
    author = {Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and
              Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
    journal = {arXiv preprint arXiv:2407.07895},
    year = {2024},
}

@article{cho2025PerceptionLM,
    title = {PerceptionLM: Open-Access Data and Models for Detailed Visual
             Understanding},
    author = {Jang Hyun Cho and Andrea Madotto and Effrosyni Mavroudi and
              Triantafyllos Afouras and Tushar Nagarajan and Muhammad Maaz and
              Yale Song and Tengyu Ma and Shuming Hu and Hanoona Rasheed and
              Peize Sun and Po-Yao Huang and Daniel Bolya and Suyog Jain and
              Miguel Martin and Huiyu Wang and Nikhila Ravi and Shashank Jain and
              Temmy Stark and Shane Moon and Babak Damavandi and Vivian Lee and
              Andrew Westbury and Salman Khan and Philipp Kr\"{a}henb\"{u}hl and
              Piotr Doll{\'a}r and Lorenzo Torresani and Kristen Grauman and
              Christoph Feichtenhofer},
    journal = {arXiv:2504.13180},
    year = {2025},
}

@misc{chen2024gmaimmbenchcomprehensivemultimodalevaluation,
    title = {GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark
             Towards General Medical AI},
    author = {Pengcheng Chen and Jin Ye and Guoan Wang and Yanjun Li and
              Zhongying Deng and Wei Li and Tianbin Li and Haodong Duan and Ziyan
              Huang and Yanzhou Su and Benyou Wang and Shaoting Zhang and Bin Fu
              and Jianfei Cai and Bohan Zhuang and Eric J Seibel and Junjun He
              and Yu Qiao},
    year = {2024},
    eprint = {2408.03361},
    archivePrefix = {arXiv},
    primaryClass = {eess.IV},
    url = {https://arxiv.org/abs/2408.03361},
}

@misc{sellergren2025medgemmatechnicalreport,
    title = {MedGemma Technical Report},
    author = {Andrew Sellergren and Sahar Kazemzadeh and Tiam Jaroensri and
              Atilla Kiraly and Madeleine Traverse and Timo Kohlberger and Shawn
              Xu and Fayaz Jamil and Cían Hughes and Charles Lau and Justin Chen
              and Fereshteh Mahvar and Liron Yatziv and Tiffany Chen and Bram
              Sterling and Stefanie Anna Baby and Susanna Maria Baby and Jeremy
              Lai and Samuel Schmidgall and Lu Yang and Kejia Chen and Per
              Bjornsson and Shashir Reddy and Ryan Brush and Kenneth Philbrick
              and Mercy Asiedu and Ines Mezerreg and Howard Hu and Howard Yang
              and Richa Tiwari and Sunny Jansen and Preeti Singh and Yun Liu and
              Shekoofeh Azizi and Aishwarya Kamath and Johan Ferret and Shreya
              Pathak and Nino Vieillard and Ramona Merhej and Sarah Perrin and
              Tatiana Matejovicova and Alexandre Ramé and Morgane Riviere and
              Louis Rouillard and Thomas Mesnard and Geoffrey Cideron and
              Jean-bastien Grill and Sabela Ramos and Edouard Yvinec and Michelle
              Casbon and Elena Buchatskaya and Jean-Baptiste Alayrac and Dmitry
              Lepikhin and Vlad Feinberg and Sebastian Borgeaud and Alek Andreev
              and Cassidy Hardin and Robert Dadashi and Léonard Hussenot and
              Armand Joulin and Olivier Bachem and Yossi Matias and Katherine
              Chou and Avinatan Hassidim and Kavi Goel and Clement Farabet and
              Joelle Barral and Tris Warkentin and Jonathon Shlens and David
              Fleet and Victor Cotruta and Omar Sanseviero and Gus Martins and
              Phoebe Kirk and Anand Rao and Shravya Shetty and David F. Steiner
              and Can Kirmizibayrak and Rory Pilgrim and Daniel Golden and Lin
              Yang},
    year = {2025},
    eprint = {2507.05201},
    archivePrefix = {arXiv},
    primaryClass = {cs.AI},
    url = {https://arxiv.org/abs/2507.05201},
}

@misc{liu2023visualinstructiontuning,
    title = {Visual Instruction Tuning},
    author = {Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
    year = {2023},
    eprint = {2304.08485},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV},
    url = {https://arxiv.org/abs/2304.08485},
}

@misc{vaswani2023attentionneed,
    title = {Attention Is All You Need},
    author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob
              Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and
              Illia Polosukhin},
    year = {2023},
    eprint = {1706.03762},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/1706.03762},
}

@misc{shi2025eagleexploringdesignspace,
    title = {Eagle: Exploring The Design Space for Multimodal LLMs with Mixture
             of Encoders},
    author = {Min Shi and Fuxiao Liu and Shihao Wang and Shijia Liao and
              Subhashree Radhakrishnan and Yilin Zhao and De-An Huang and Hongxu
              Yin and Karan Sapra and Yaser Yacoob and Humphrey Shi and Bryan
              Catanzaro and Andrew Tao and Jan Kautz and Zhiding Yu and Guilin
              Liu},
    year = {2025},
    eprint = {2408.15998},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV},
    url = {https://arxiv.org/abs/2408.15998},
}

@misc{bai2025qwen3vltechnicalreport,
    title = {Qwen3-VL Technical Report},
    author = {Shuai Bai and Yuxuan Cai and Ruizhe Chen and Keqin Chen and
              Xionghui Chen and Zesen Cheng and Lianghao Deng and Wei Ding and
              Chang Gao and Chunjiang Ge and Wenbin Ge and Zhifang Guo and Qidong
              Huang and Jie Huang and Fei Huang and Binyuan Hui and Shutong Jiang
              and Zhaohai Li and Mingsheng Li and Mei Li and Kaixin Li and
              Zicheng Lin and Junyang Lin and Xuejing Liu and Jiawei Liu and
              Chenglong Liu and Yang Liu and Dayiheng Liu and Shixuan Liu and
              Dunjie Lu and Ruilin Luo and Chenxu Lv and Rui Men and Lingchen
              Meng and Xuancheng Ren and Xingzhang Ren and Sibo Song and Yuchong
              Sun and Jun Tang and Jianhong Tu and Jianqiang Wan and Peng Wang
              and Pengfei Wang and Qiuyue Wang and Yuxuan Wang and Tianbao Xie
              and Yiheng Xu and Haiyang Xu and Jin Xu and Zhibo Yang and Mingkun
              Yang and Jianxin Yang and An Yang and Bowen Yu and Fei Zhang and
              Hang Zhang and Xi Zhang and Bo Zheng and Humen Zhong and Jingren
              Zhou and Fan Zhou and Jing Zhou and Yuanzhi Zhu and Ke Zhu},
    year = {2025},
    eprint = {2511.21631},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV},
    url = {https://arxiv.org/abs/2511.21631},
}

@misc{openai2025gpt5,
    author = {OpenAI},
    title = {Introducing GPT-5},
    year = {2025},
    howpublished = {\url{https://openai.com/index/introducing-gpt-5/}},
    note = {Accessed: 2025-12},
}

@misc{google2025gemini3,
    author = {Google DeepMind},
    title = {Gemini 3},
    year = {2025},
    howpublished = {\url{https://deepmind.google/models/gemini/}},
    note = {Accessed: 2025-12},
}

@misc{liu2024llavanext,
    title = {LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url = {https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang,
              Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month = {January},
    year = {2024},
}

@misc{mistral2025mistral3,
    author = {Mistral},
    title = {Introducing Mistral 3},
    year = {2025},
    howpublished = {\url{https://mistral.ai/fr/news/mistral-3}},
    note = {Accessed: 2025-12},
}

@misc{moor2023medflamingomultimodalmedicalfewshot,
    title = {Med-Flamingo: a Multimodal Medical Few-shot Learner},
    author = {Michael Moor and Qian Huang and Shirley Wu and Michihiro Yasunaga
              and Cyril Zakka and Yash Dalmia and Eduardo Pontes Reis and Pranav
              Rajpurkar and Jure Leskovec},
    year = {2023},
    eprint = {2307.15189},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV},
    url = {https://arxiv.org/abs/2307.15189},
}

@misc{meta2025llama4,
    author = {Meta AI},
    title = {The Llama 4 herd: The beginning of a new era of natively multimodal
             AI innovation},
    year = {2025},
    howpublished = {\url{
                    https://ai.meta.com/blog/llama-4-multimodal-intelligence/}},
    note = {Accessed: 2025-12},
}

@misc{cho2025perceptionlmopenaccessdatamodels,
    title = {PerceptionLM: Open-Access Data and Models for Detailed Visual
             Understanding},
    author = {Jang Hyun Cho and Andrea Madotto and Effrosyni Mavroudi and
              Triantafyllos Afouras and Tushar Nagarajan and Muhammad Maaz and
              Yale Song and Tengyu Ma and Shuming Hu and Suyog Jain and Miguel
              Martin and Huiyu Wang and Hanoona Rasheed and Peize Sun and Po-Yao
              Huang and Daniel Bolya and Nikhila Ravi and Shashank Jain and Tammy
              Stark and Shane Moon and Babak Damavandi and Vivian Lee and Andrew
              Westbury and Salman Khan and Philipp Krähenbühl and Piotr Dollár
              and Lorenzo Torresani and Kristen Grauman and Christoph
              Feichtenhofer},
    year = {2025},
    eprint = {2504.13180},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV},
    url = {https://arxiv.org/abs/2504.13180},
}

@misc{li2023llavamedtraininglargelanguageandvision,
    title = {LLaVA-Med: Training a Large Language-and-Vision Assistant for
             Biomedicine in One Day},
    author = {Chunyuan Li and Cliff Wong and Sheng Zhang and Naoto Usuyama and
              Haotian Liu and Jianwei Yang and Tristan Naumann and Hoifung Poon
              and Jianfeng Gao},
    year = {2023},
    eprint = {2306.00890},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV},
    url = {https://arxiv.org/abs/2306.00890},
}
