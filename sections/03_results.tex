% For each objective listed in Section \ref{sec:aim}, present your results in a dedicated subsection. Within each subsection:
%
% \subsection{Results for objective 1}
% \label{sec:results1}
%
% Present your findings in a logical sequence. Begin with descriptive statistics or baseline results. Follow with your main analyses. Include relevant tables and figures, ensuring each is self-explanatory with comprehensive captions.
%
% \subsection{Results for objective 2}
% \label{sec:results2}
%
% Present results that directly address your second objective. Include statistical analyses and significance levels. Compare your results with relevant benchmarks or baselines.
%
% \subsection{Results for objective 3}
% \label{sec:results3}
%
% Document the outcomes of your methodology development. Include performance metrics and validation results. Present any ablation studies or sensitivity analyses performed.
%
% \subsection{Results for objective 4}
% \label{sec:results4}
%
% Present comparative analyses between your method and existing approaches. Use statistical tests to support your conclusions. Include measures of practical significance.
% \begin{table}[htb]
%     \centering
%     \caption{Pulmonary function test results}
%     \label{tab:pft_results}
%     \begin{tabular}{@{}llcccc@{}}
%         \toprule
%         \textbf{Group} & \textbf{Base model}
%         & & \multicolumn{1}{c}{\textit{Value}} & \multicolumn{1}{c}{\textit{\% Pred}} & \multicolumn{1}{c}{\textit{Value}} & \multicolumn{1}{c}{\textit{\% Pred}} \\
%         \hline
%         \multirow{3}{*}{Open models}
%             & FEV\textsubscript{1} (L) & 3.82 & 98.5 & 3.90 & 100.2 \\
%             & FVC (L) & 4.75 & 97.3 & 4.80 & 98.4 \\
%             & FEV\textsubscript{1}/FVC (\%) & 80.4 & -- & 81.3 & -- \\
%         \hline
%         \multirow{3}{*}{Other models}
%             & FEV\textsubscript{1} (L) & 2.15 & 55.4 & 2.45 & 63.1 \\
%             & FVC (L) & 3.52 & 72.1 & 3.78 & 77.5 \\
%             & FEV\textsubscript{1}/FVC (\%) & 61.1 & -- & 64.8 & -- \\
%         \bottomrule
%     \end{tabular}
% \end{table}

\section{Results}
\label{sec:results}

\subsection{Loglikelihood on aligned models}

This section presents

\begin{table}

\end{table}

\subsection{End-to-End results}

\begin{table}
    \centering
    \setlength{\tabcolsep}{2.2pt}
    \renewcommand{\arraystretch}{1.4}
    \begin{tabular}{l|c|ccc|ccc}
        \hline
        \multirow{2}{*}{\textbf{Model name}} & \multirow{2}{*}{\textbf{GMAI}} & \multicolumn{3}{c|}{\textbf{PathVQA}} & \multicolumn{3}{c}{\textbf{SLAKE}} \\
                                             &                                & yes/no & open-ended & overall        & yes/no & open-ended & overall         \\
                                                  \hline
                                                  \multicolumn{8}{l}{\textbf{Open weights}} \\
                                                  \hline
        MedGemma 4B                               & \textbf{44.9}                  & \textbf{64.9} & 4.1            & \textbf{34.5}    & 50            & \textbf{32.4}   & \textbf{33.8}   \\
        Perception-LM 8B                          & 42.2                           & 56.1          & 1.4            & 28.7             & 52.1          & 27.8            & 29.9            \\
        MultiMeditron Qwen3-4B BiomedCLIP         & 35.3                           & 57.4          & 2.4            & 29.9             & \textbf{55.6} & 27.7            & 30.1            \\
        MultiMeditron Llama3.1 8B BiomedCLIP      & 36.6                           & 55.7          & 3.4            & 29.5             & 48.1          & 22.4            & 24.5            \\
        MultiMeditron Llama3.1 8B CLIP            & 34                             & 60.6          & \textbf{5.6}   & 33.1             & 50.5          & 28.5            & 30.3            \\
        MultiMeditron Llama 3.1 8B ATTN-PEP       & 29.6                           & 59.1          & 1.5            & 30.3             & 51.1          & 27.6            & 29.6            \\
        MultiMeditron Llama 3.1 8B ATTN-SHARED    & 28.6                           & 56.9          & 2              & 29.5             & 46            & 25.8            & 27.5            \\
        MultiMeditron Llama 3.1 8B AVG-PEP        & 30.7                           & 46.5          & 2.5            & 24.5             & 47.6          & 25.8            & 27.6            \\
        MultiMeditron Llama 3.1 8B ATTN-SHARED    & 29.7                           & 46.8          & 2.6            & 24.2             & 49.5          & 23.7            & 25.8            \\
        \hline
        \multicolumn{8}{l}{\textbf{Fully Open}}                                                                                                                \\
        \hline
        Molmo2-O-7B                               & \textbf{36.7}                  & \textbf{60.5} & \textbf{2.3}   & \textbf{31.5}    & \textbf{53.7}  & \textbf{26.6}   & \textbf{28.8} \\
        MultiMeditron Apertus-8B BiomedCLIP                     & 34.2             & 57.4          & 1.2            & 29.9             & 51.3           & 21              & 23.6            \\
        \hline
    \end{tabular}
    \caption{Evaluation of MultiMeditron after the end-to-end phase and other multimodal models on medical image–text benchmarks. Here. we report the accuracy in percentage for each benchmark. For open-ended questions, an answer is correct if it exactly matches the expected answer. The overall columns show the combined accuracy over yes/no and open-ended questions for the given benchmark}
    \label{tab:eval}
\end{table}

This section presents the quantitative evaluation of the proposed multimodal medical language models after the end-to-end training phase on three widely used medical vision–language benchmarks: GMAI, PathVQA, and SLAKE.

All results are reported in terms of accuracy, computed using an exact-match criterion between the model prediction and the ground-truth answer. GMAI is formulated as a multiple-choice task, while PathVQA and SLAKE contain both yes/no and open-ended questions. We show the results of our evaluation on these benchmarks in Table \ref{tab:eval}



